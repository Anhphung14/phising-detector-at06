{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13ccf6a8-b42b-4023-b567-4f7fbf08bed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 1: Khai báo và cấu hình hoàn tất.\n",
      "Sẽ xử lý 4 file từ thư mục: '../data/dataForTrain'\n"
     ]
    }
   ],
   "source": [
    "# --- 1. IMPORT CÁC THƯ VIỆN CẦN THIẾT ---\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from scipy.sparse import hstack\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os # Thêm thư viện os để làm việc với đường dẫn file\n",
    "\n",
    "# --- 2. CẤU HÌNH CHO NOTEBOOK ---\n",
    "# Cấu hình cho biểu đồ đẹp hơn\n",
    "sns.set_style('whitegrid')\n",
    "# Lệnh magic để hiển thị biểu đồ ngay trong notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# --- 3. CÁC HẰNG SỐ VÀ BIẾN TOÀN CỤC ---\n",
    "# Đặt một seed cố định để đảm bảo kết quả có thể tái lập\n",
    "RANDOM_SEED = 42\n",
    "# Đường dẫn đến thư mục chứa dữ liệu\n",
    "data_folder = '../data/dataForTrain'\n",
    "# Danh sách các file CSV sẽ được xử lý\n",
    "files_to_process = ['CEAS_08.csv', 'Nazario.csv', 'Nigerian_Fraud.csv', 'SpamAssasin.csv']\n",
    "\n",
    "print(\"Cell 1: Khai báo và cấu hình hoàn tất.\")\n",
    "print(f\"Sẽ xử lý {len(files_to_process)} file từ thư mục: '{data_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "098cd740-48a9-49a9-ba2e-52ed6dd7964f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu quá trình đọc và hợp nhất 4 file...\n",
      "\n",
      "--- Đang xử lý file: CEAS_08.csv ---\n",
      "-> Đọc thành công. Số dòng: 39154\n",
      "\n",
      "--- Đang xử lý file: Nazario.csv ---\n",
      "-> Đọc thành công. Số dòng: 1565\n",
      "\n",
      "--- Đang xử lý file: Nigerian_Fraud.csv ---\n",
      "-> Đọc thành công. Số dòng: 3332\n",
      "\n",
      "--- Đang xử lý file: SpamAssasin.csv ---\n",
      "-> Đọc thành công. Số dòng: 5809\n",
      "\n",
      "--- Gộp dữ liệu hoàn tất ---\n",
      "Tổng số dòng trước khi làm sạch: 49860\n",
      "Tổng số dòng sau khi làm sạch cơ bản: 49772\n",
      "\n",
      "--- Thông tin các cột của Master DataFrame ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 49772 entries, 0 to 49859\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sender    49446 non-null  object\n",
      " 1   receiver  47706 non-null  object\n",
      " 2   date      49295 non-null  object\n",
      " 3   subject   49772 non-null  object\n",
      " 4   body      49772 non-null  object\n",
      " 5   label     49772 non-null  int64 \n",
      " 6   urls      49772 non-null  int64 \n",
      " 7   source    49772 non-null  object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Tải và Hợp nhất Dữ liệu Đa trường\n",
    "\n",
    "all_dfs = [] # List để chứa các DataFrame của từng file\n",
    "print(\"Bắt đầu quá trình đọc và hợp nhất 4 file...\")\n",
    "\n",
    "for file_name in files_to_process:\n",
    "    file_path = os.path.join(data_folder, file_name)\n",
    "    print(f\"\\n--- Đang xử lý file: {file_name} ---\")\n",
    "    try:\n",
    "        # Đọc file CSV, sử dụng encoding='latin1' để tránh lỗi ký tự đặc biệt\n",
    "        df_temp = pd.read_csv(file_path, encoding='latin1')\n",
    "        \n",
    "        # Thêm cột 'source' để biết nguồn gốc của mỗi dòng dữ liệu\n",
    "        df_temp['source'] = file_name\n",
    "        \n",
    "        all_dfs.append(df_temp)\n",
    "        print(f\"-> Đọc thành công. Số dòng: {len(df_temp)}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"-> LỖI: Không tìm thấy file {file_path}. Bỏ qua.\")\n",
    "    except Exception as e:\n",
    "        print(f\"-> LỖI khi xử lý file {file_path}: {e}. Bỏ qua.\")\n",
    "\n",
    "# Gộp tất cả các DataFrame trong list lại thành một DataFrame tổng\n",
    "if all_dfs:\n",
    "    master_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    print(\"\\n--- Gộp dữ liệu hoàn tất ---\")\n",
    "    print(f\"Tổng số dòng trước khi làm sạch: {len(master_df)}\")\n",
    "    \n",
    "    # --- LÀM SẠCH CƠ BẢN ---\n",
    "    # 1. Chỉ giữ lại các cột cần thiết, đảm bảo một cấu trúc đồng nhất\n",
    "    # (Loại bỏ các cột không xác định nếu có)\n",
    "    required_columns = ['sender', 'receiver', 'date', 'subject', 'body', 'label', 'urls', 'source']\n",
    "    master_df = master_df[required_columns]\n",
    "    \n",
    "    # 2. Xử lý các giá trị bị thiếu (chỉ trên các cột quan trọng)\n",
    "    master_df.dropna(subset=['subject', 'body', 'label'], inplace=True)\n",
    "    \n",
    "    # 3. Loại bỏ các dòng bị trùng lặp dựa trên nội dung\n",
    "    master_df.drop_duplicates(subset=['subject', 'body'], inplace=True, keep='first')\n",
    "    \n",
    "    # 4. Chuyển đổi kiểu dữ liệu cho chuẩn\n",
    "    master_df['label'] = master_df['label'].astype(int)\n",
    "\n",
    "    print(f\"Tổng số dòng sau khi làm sạch cơ bản: {len(master_df)}\")\n",
    "    print(\"\\n--- Thông tin các cột của Master DataFrame ---\")\n",
    "    master_df.info()\n",
    "else:\n",
    "    print(\"\\nKhông có dữ liệu để xử lý. Vui lòng kiểm tra lại các file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71056f6e-f2c1-4751-b1aa-9992ee7ee466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bắt đầu trích xuất và phân tích đặc trưng ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m     date_str_cleaned = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+[A-Z]\u001b[39m\u001b[33m{\u001b[39m\u001b[33m2,5}$\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, date_str.strip())\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.to_datetime(date_str_cleaned, utc=\u001b[38;5;28;01mTrue\u001b[39;00m, errors=\u001b[33m'\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m parsed_dates = \u001b[43mdf_features\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msafe_parse_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m df_features[\u001b[33m'\u001b[39m\u001b[33mday_of_week\u001b[39m\u001b[33m'\u001b[39m] = parsed_dates.dt.dayofweek\n\u001b[32m     25\u001b[39m df_features[\u001b[33m'\u001b[39m\u001b[33mhour_of_day\u001b[39m\u001b[33m'\u001b[39m] = parsed_dates.dt.hour\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\phishing-detector\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\phishing-detector\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\phishing-detector\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\phishing-detector\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\phishing-detector\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36msafe_parse_date\u001b[39m\u001b[34m(date_str)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msafe_parse_date\u001b[39m(date_str):\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(date_str, \u001b[38;5;28mstr\u001b[39m): \u001b[38;5;28;01mreturn\u001b[39;00m pd.NaT\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     date_str_cleaned = \u001b[43mre\u001b[49m.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+[A-Z]\u001b[39m\u001b[33m{\u001b[39m\u001b[33m2,5}$\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, date_str.strip())\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.to_datetime(date_str_cleaned, utc=\u001b[38;5;28;01mTrue\u001b[39;00m, errors=\u001b[33m'\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 3 (Phiên bản Hoàn chỉnh)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if 'master_df' in locals() and not master_df.empty:\n",
    "    print(\"--- Bắt đầu trích xuất và phân tích đặc trưng ---\")\n",
    "    \n",
    "    df_features = master_df.copy()\n",
    "\n",
    "    # --- 1. TẠO CÁC ĐẶC TRƯNG TỪ CÁC CỘT GỐC ---\n",
    "    \n",
    "    # a. Đặc trưng từ 'subject' và 'body'\n",
    "    df_features['text_combined'] = df_features['subject'].astype(str) + ' ' + df_features['body'].astype(str)\n",
    "    df_features['char_count'] = df_features['text_combined'].apply(len)\n",
    "    df_features['word_count'] = df_features['text_combined'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    # b. Đặc trưng từ 'date'\n",
    "    def safe_parse_date(date_str):\n",
    "        if not isinstance(date_str, str): return pd.NaT\n",
    "        date_str_cleaned = re.sub(r'\\s+[A-Z]{2,5}$', '', date_str.strip())\n",
    "        return pd.to_datetime(date_str_cleaned, utc=True, errors='coerce')\n",
    "    \n",
    "    parsed_dates = df_features['date'].apply(safe_parse_date)\n",
    "    df_features['day_of_week'] = parsed_dates.dt.dayofweek\n",
    "    df_features['hour_of_day'] = parsed_dates.dt.hour\n",
    "    \n",
    "    # Điền các giá trị thiếu bằng giá trị trung vị\n",
    "    df_features.fillna({\n",
    "        'day_of_week': df_features['day_of_week'].median(),\n",
    "        'hour_of_day': df_features['hour_of_day'].median()\n",
    "    }, inplace=True)\n",
    "\n",
    "    # c. Đặc trưng từ 'sender' và 'receiver'\n",
    "    def get_domain(email_address):\n",
    "        if not isinstance(email_address, str) or '@' not in email_address:\n",
    "            return 'unknown'\n",
    "        # Chỉ lấy phần domain cấp cao nhất (ví dụ: 'a.b.com' -> 'com') để giảm số lượng danh mục\n",
    "        domain_parts = email_address.split('@')[-1].split('.')\n",
    "        return domain_parts[-1] if len(domain_parts) > 1 else 'unknown'\n",
    "\n",
    "    df_features['sender_domain'] = df_features['sender'].apply(get_domain)\n",
    "    df_features['receiver_domain'] = df_features['receiver'].apply(get_domain)\n",
    "    df_features['same_domain'] = (df_features['sender_domain'] == df_features['receiver_domain']).astype(int)\n",
    "    \n",
    "    print(\"Trích xuất đặc trưng từ tất cả các trường hoàn tất!\")\n",
    "    \n",
    "    # --- 2. PHÂN TÍCH DỮ LIỆU TRỰC QUAN (EDA) ---\n",
    "    \n",
    "    phishing_df = df_features[df_features['label'] == 1]\n",
    "    safe_df = df_features[df_features['label'] == 0]\n",
    "    features_to_analyze = ['urls', 'char_count', 'word_count', 'hour_of_day', 'same_domain']\n",
    "    \n",
    "    print(\"\\n--- Vẽ biểu đồ phân bổ các đặc trưng ---\")\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    for i, feature in enumerate(features_to_analyze):\n",
    "        plt.subplot(3, 2, i + 1)\n",
    "        if feature == 'same_domain':\n",
    "            sns.countplot(data=df_features, x=feature, hue='label')\n",
    "        else:\n",
    "            sns.histplot(safe_df[feature], color='blue', label='Safe', kde=True, stat=\"density\", linewidth=0)\n",
    "            sns.histplot(phishing_df[feature], color='red', label='Phishing', kde=True, stat=\"density\", linewidth=0)\n",
    "            plt.legend()\n",
    "        plt.title(f'Phân bổ của \"{feature}\"')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Master DataFrame chưa được tạo. Vui lòng chạy Cell 2 trước.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e07406a7-115a-43f6-98a5-5650941825ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chuẩn bị dữ liệu cho mô hình ---\n",
      "Vector hóa văn bản bằng TF-IDF...\n",
      "Lấy các đặc trưng số...\n",
      "Kết hợp các loại đặc trưng...\n",
      "\n",
      "Chuẩn bị dữ liệu hoàn tất!\n",
      "Dữ liệu huấn luyện cuối cùng có dạng (hàng, cột): (39817, 5005)\n"
     ]
    }
   ],
   "source": [
    "if 'df_features' in locals() and not df_features.empty:\n",
    "    print(\"--- Chuẩn bị dữ liệu cho mô hình ---\")\n",
    "    \n",
    "    # Xác định các cột đặc trưng và cột nhãn\n",
    "    labels = df_features['label']\n",
    "    # Bỏ các cột không cần thiết cho việc huấn luyện\n",
    "    features_df = df_features.drop(columns=['sender', 'receiver', 'date', 'subject', 'body', 'label', 'source'])\n",
    "\n",
    "    # Chia dữ liệu thành tập huấn luyện (train) và kiểm thử (test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_df, \n",
    "        labels, \n",
    "        test_size=0.2, \n",
    "        random_state=RANDOM_SEED, \n",
    "        stratify=labels\n",
    "    )\n",
    "    \n",
    "    # --- Xử lý các loại đặc trưng khác nhau ---\n",
    "    \n",
    "    # a. Vector hóa cột văn bản ('text_combined') bằng TF-IDF\n",
    "    print(\"Vector hóa văn bản bằng TF-IDF...\")\n",
    "    tfidf = TfidfVectorizer(max_features=5000) # Lấy 5000 từ quan trọng nhất\n",
    "    X_train_text_tfidf = tfidf.fit_transform(X_train['text_combined'])\n",
    "    X_test_text_tfidf = tfidf.transform(X_test['text_combined'])\n",
    "    \n",
    "    # b. Lấy các cột đặc trưng số còn lại\n",
    "    print(\"Lấy các đặc trưng số...\")\n",
    "    numeric_features_cols = ['urls', 'char_count', 'word_count', 'day_of_week', 'hour_of_day']\n",
    "    X_train_numeric = X_train[numeric_features_cols].values\n",
    "    X_test_numeric = X_test[numeric_features_cols].values\n",
    "    \n",
    "    # c. Kết hợp tất cả các đặc trưng lại\n",
    "    print(\"Kết hợp các loại đặc trưng...\")\n",
    "    X_train_final = hstack([X_train_text_tfidf, X_train_numeric]).tocsr()\n",
    "    X_test_final = hstack([X_test_text_tfidf, X_test_numeric]).tocsr()\n",
    "    \n",
    "    print(f\"\\nChuẩn bị dữ liệu hoàn tất!\")\n",
    "    print(f\"Dữ liệu huấn luyện cuối cùng có dạng (hàng, cột): {X_train_final.shape}\")\n",
    "else:\n",
    "    print(\"DataFrame df_features chưa được tạo. Vui lòng chạy Cell 3 trước.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868581c8-bbf1-4f8f-9b64-2cfc95fa949c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
